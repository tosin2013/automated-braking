{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37c26e5-b837-4337-9a0b-ef200370865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import kfp\n",
    "import kfp.client\n",
    "from kfp import dsl\n",
    "from kfp import compiler\n",
    "from kfp import kubernetes\n",
    "\n",
    "\n",
    "PIPELINE_NAME = \"distance-prediction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0301c74-f0bd-4a6c-bca4-3543b71860e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=\"image-registry.openshift-image-registry.svc:5000/redhat-ods-applications/runtime-tensorflow:tensorflow\",\n",
    "    packages_to_install=[\"psycopg2\", \"onnx==1.17.0\", \"onnxruntime==1.19.2\", \"tf2onnx==1.16.1\"]\n",
    ")\n",
    "def train_model(fresh_run: str, output_path: dsl.OutputPath(bytes)):\n",
    "    from psycopg2 import connect\n",
    "    from os import getenv\n",
    "    from pandas import read_sql_query\n",
    "\n",
    "    print(fresh_run)\n",
    "    try:\n",
    "        conn = connect(dbname=getenv('DBNAME'), user=getenv('USER'), host=getenv('HOST'), password=getenv('PASSWORD'))\n",
    "    except Exception as e:\n",
    "        print(\"I am unable to connect to the database\")\n",
    "        print(e)\n",
    "        raise e\n",
    "    \n",
    "    sql_query = \"SELECT * FROM distance.approaching_vehicle WHERE EVENT_TIMESTAMP < NOW()\"\n",
    "    data = read_sql_query(sql_query, con=conn)\n",
    "    conn.close()\n",
    "\n",
    "    print(data.head())\n",
    "\n",
    "    from numpy import random\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    random.default_rng(seed=513421)\n",
    "    \n",
    "    train_columns = [\n",
    "        \"other_bottom\",\n",
    "        \"other_left\",\n",
    "        \"other_right\",\n",
    "        \"other_top\",\n",
    "        \"other_speed\",\n",
    "        \"your_speed\"\n",
    "    ]\n",
    "    prediction_column = \"brake_amount\"\n",
    "    \n",
    "    x = data[train_columns].values\n",
    "    y = data[prediction_column].values\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "    print(x_train.shape, x_test.shape)\n",
    "\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_dim=len(train_columns)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Train the model and get performance\n",
    "    import os\n",
    "    import time\n",
    "    \n",
    "    start = time.time()\n",
    "    epochs = 2\n",
    "    history = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        validation_data=(x_test, y_test),\n",
    "        verbose=True\n",
    "    )\n",
    "    end = time.time()\n",
    "    print(f\"Training of model is complete. Took {end-start} seconds\")\n",
    "\n",
    "    import tensorflow as tf\n",
    "    import tf2onnx\n",
    "    import onnx\n",
    "    import pickle\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Normally we use tf2.onnx.convert.from_keras.\n",
    "    # workaround for tf2onnx bug https://github.com/onnx/tensorflow-onnx/issues/2348\n",
    "    \n",
    "    # Wrap the model in a `tf.function`\n",
    "    @tf.function(input_signature=[tf.TensorSpec([None, x_train.shape[1]], tf.float32, name='dense_input')])\n",
    "    def model_fn(x):\n",
    "        return model(x)\n",
    "    \n",
    "    # Convert the Keras model to ONNX\n",
    "    model_proto, _ = tf2onnx.convert.from_function(\n",
    "        model_fn,\n",
    "        input_signature=[tf.TensorSpec([None, x_train.shape[1]], tf.float32, name='dense_input')]\n",
    "    )\n",
    "\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        f.write(model_proto.SerializeToString())\n",
    "    print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616b3ac6-2887-40a8-83aa-0e24a2877ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=\"image-registry.openshift-image-registry.svc:5000/redhat-ods-applications/runtime-tensorflow:tensorflow\",\n",
    "    packages_to_install=[\"boto3==1.35.55\", \"botocore==1.35.55\"]\n",
    ")\n",
    "def save_model(fresh_run: str, input_path: dsl.InputPath(bytes)) -> str:\n",
    "    import os\n",
    "    import boto3\n",
    "    import botocore\n",
    "\n",
    "    print(fresh_run)\n",
    "    print(\"Starting to save the model\")\n",
    "    aws_access_key_id = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "    aws_secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "    endpoint_url = os.environ.get('AWS_S3_ENDPOINT')\n",
    "    region_name = os.environ.get('AWS_DEFAULT_REGION')\n",
    "    bucket_name = os.environ.get('AWS_S3_BUCKET')\n",
    "\n",
    "    missing_var = False\n",
    "    for aws_var in ['AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY', 'AWS_S3_ENDPOINT', 'AWS_DEFAULT_REGION', 'AWS_S3_BUCKET']:\n",
    "        if not os.environ.get(aws_var):\n",
    "            print(aws_var)\n",
    "            missing_var = True\n",
    "    if missing_var:\n",
    "        raise ValueError(\"One or more connection variables are empty.  \"\n",
    "                         \"Please check your connection to an S3 bucket.\")\n",
    "    \n",
    "    session = boto3.session.Session(aws_access_key_id=aws_access_key_id,\n",
    "                                    aws_secret_access_key=aws_secret_access_key)\n",
    "    \n",
    "    s3_resource = session.resource(\n",
    "        's3',\n",
    "        config=botocore.client.Config(signature_version='s3v4'),\n",
    "        endpoint_url=endpoint_url,\n",
    "        region_name=region_name)\n",
    "    \n",
    "    bucket = s3_resource.Bucket(bucket_name)\n",
    "    print(\"Got the connection to the bucket\")\n",
    "    \n",
    "    def upload_model_to_s3(local_file, s3_prefix):\n",
    "        next_version = get_next_version(list_objects(\"models\"))\n",
    "        file_path = os.path.join('distance', next_version, \"model.onnx\")\n",
    "        s3_key = os.path.join(s3_prefix, file_path)\n",
    "        os.makedirs(\"/tmp/models/distance\", exist_ok=True)\n",
    "        with open(local_file, 'rb') as f:\n",
    "            data = f.read()\n",
    "            with open(\"/tmp/models/distance/model.onnx\", 'wb') as tmp:\n",
    "                tmp.write(data)\n",
    "            \n",
    "        bucket.upload_file(\"/tmp/models/distance/model.onnx\", s3_key)\n",
    "        return next_version\n",
    "    \n",
    "    \n",
    "    def list_objects(prefix):\n",
    "        objects = []\n",
    "        filter = bucket.objects.filter(Prefix=prefix)\n",
    "        for obj in filter.all():\n",
    "            objects.append(obj.key)\n",
    "        return objects\n",
    "    \n",
    "    \n",
    "    def get_next_version(models):\n",
    "        model_versions = [int(x.split('models/distance/')[1].split('/')[0]) for x in models]\n",
    "        next_version = str(max(model_versions) + 1)\n",
    "        return next_version\n",
    "    \n",
    "    next_version = upload_model_to_s3(input_path, \"models\")\n",
    "    print(\"Finished uploading the model\")\n",
    "    return next_version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8e094c-292f-454e-963f-4b7c741913ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=\"image-registry.openshift-image-registry.svc:5000/redhat-ods-applications/runtime-tensorflow:tensorflow\",\n",
    "    packages_to_install=[\"boto3==1.35.55\", \"botocore==1.35.55\"]\n",
    ")\n",
    "def publish_to_registry(user_token: str, version: str):\n",
    "    from requests import get, post\n",
    "    from os import getenv\n",
    "\n",
    "    headers = { \"Authorization\": f\"Bearer {user_token}\" }\n",
    "    base_url = getenv(\"BASE_URL\")\n",
    "\n",
    "    model_name = 'distance'\n",
    "    response = get(url=f\"https://distance-prediction-rest{base_url}/api/model_registry/v1alpha3/registered_models\",\n",
    "                   headers=headers)\n",
    "    print(response)\n",
    "    print(\"Creating the model if we need to\")\n",
    "    models = response.json()['items']\n",
    "    print(models)\n",
    "    model_exists = [x for x in models if x['name'] == model_name]\n",
    "    print(model_exists)\n",
    "    \n",
    "    if not model_exists:\n",
    "        data = {\n",
    "            \"name\": model_name\n",
    "        }\n",
    "        \n",
    "        new_model = post(url=f\"https://distance-prediction-rest{base_url}/api/model_registry/v1alpha3/registered_models\",\n",
    "                   headers=headers, json=data)\n",
    "        new_model\n",
    "        print(\"Created new model\")\n",
    "        model_id = new_model.json()['id']\n",
    "    else:\n",
    "        model_id = model_exists[0]['id']\n",
    "    print(model_id)\n",
    "\n",
    "    data = {\n",
    "        \"author\": \"kube:admin\",\n",
    "        \"name\": version,\n",
    "        \"registeredModelId\": model_id,\n",
    "        \"state\": \"LIVE\"\n",
    "    }\n",
    "    \n",
    "    print(\"Creating the model version\", version)\n",
    "    response = post(url=f\"https://distance-prediction-rest{base_url}/api/model_registry/v1alpha3/model_versions\",\n",
    "                   headers=headers,\n",
    "                   json=data)\n",
    "    print(response)\n",
    "    model_version_id = response.json()['id']\n",
    "\n",
    "    data = {\n",
    "        'artifactType': 'model-artifact',\n",
    "        \"storageKey\": \"aws-connection-my-storage\",\n",
    "        \"name\": version,\n",
    "        \"modelFormatName\": \"onnx\",\n",
    "        \"modelFormatVersion\": \"1\",\n",
    "        \"storagePath\": f\"models/distance\",\n",
    "        \"uri\": f\"s3://my-storage/models/distance/{version}?endpoint={getenv('AWS_S3_ENDPOINT')}&defaultRegion={getenv('AWS_DEFAULT_REGION')}\",\n",
    "        \"description\": \"Distance Prediction Model\",\n",
    "        \"state\": \"LIVE\"\n",
    "    }\n",
    "    \n",
    "    print(\"Creating the model artifact\")\n",
    "    response = post(url=f\"https://distance-prediction-rest{base_url}/api/model_registry/v1alpha3/model_versions/{model_version_id}/artifacts\",\n",
    "                   headers=headers,\n",
    "                   json=data)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba99ec58-8dd4-49ba-b9d7-2b8675f49321",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=PIPELINE_NAME)\n",
    "def distance_model_pipeline(user_token: str, fresh_run: str):\n",
    "    # Step 1\n",
    "    train_model_task = train_model(fresh_run=fresh_run)\n",
    "    kubernetes.use_secret_as_env(train_model_task, secret_name='postgres-creds', secret_key_to_env={\"DBNAME\": \"DBNAME\"})\n",
    "    kubernetes.use_secret_as_env(train_model_task, secret_name='postgres-creds', secret_key_to_env={\"HOST\": \"HOST\"})\n",
    "    kubernetes.use_secret_as_env(train_model_task, secret_name='postgres-creds', secret_key_to_env={\"PASSWORD\": \"PASSWORD\"})\n",
    "    kubernetes.use_secret_as_env(train_model_task, secret_name='postgres-creds', secret_key_to_env={\"USER\": \"USER\"})\n",
    "\n",
    "    # Step 2\n",
    "    save_model_task = save_model(fresh_run=fresh_run, input_path=train_model_task.output)\n",
    "    kubernetes.use_secret_as_env(save_model_task, secret_name='aws-connection-my-storage', secret_key_to_env={\"AWS_ACCESS_KEY_ID\": \"AWS_ACCESS_KEY_ID\"})\n",
    "    kubernetes.use_secret_as_env(save_model_task, secret_name='aws-connection-my-storage', secret_key_to_env={\"AWS_DEFAULT_REGION\": \"AWS_DEFAULT_REGION\"})\n",
    "    kubernetes.use_secret_as_env(save_model_task, secret_name='aws-connection-my-storage', secret_key_to_env={\"AWS_S3_BUCKET\": \"AWS_S3_BUCKET\"})\n",
    "    kubernetes.use_secret_as_env(save_model_task, secret_name='aws-connection-my-storage', secret_key_to_env={\"AWS_S3_ENDPOINT\": \"AWS_S3_ENDPOINT\"})\n",
    "    kubernetes.use_secret_as_env(save_model_task, secret_name='aws-connection-my-storage', secret_key_to_env={\"AWS_SECRET_ACCESS_KEY\": \"AWS_SECRET_ACCESS_KEY\"})\n",
    "\n",
    "    # Step 3\n",
    "    publish_task = publish_to_registry(user_token=user_token, version=save_model_task.output)\n",
    "    kubernetes.use_secret_as_env(publish_task, secret_name='aws-connection-my-storage', secret_key_to_env={\"AWS_ACCESS_KEY_ID\": \"AWS_ACCESS_KEY_ID\"})\n",
    "    kubernetes.use_secret_as_env(publish_task, secret_name='aws-connection-my-storage', secret_key_to_env={\"AWS_DEFAULT_REGION\": \"AWS_DEFAULT_REGION\"})\n",
    "    kubernetes.use_secret_as_env(publish_task, secret_name='aws-connection-my-storage', secret_key_to_env={\"AWS_S3_BUCKET\": \"AWS_S3_BUCKET\"})\n",
    "    kubernetes.use_secret_as_env(publish_task, secret_name='aws-connection-my-storage', secret_key_to_env={\"AWS_S3_ENDPOINT\": \"AWS_S3_ENDPOINT\"})\n",
    "    kubernetes.use_secret_as_env(publish_task, secret_name='aws-connection-my-storage', secret_key_to_env={\"AWS_SECRET_ACCESS_KEY\": \"AWS_SECRET_ACCESS_KEY\"})\n",
    "    kubernetes.use_secret_as_env(publish_task, secret_name='urls', secret_key_to_env={\"BASE_URL\": \"BASE_URL\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac72463-416d-4574-b8c7-2a24a5698511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the pipeline server\n",
    "from os import getenv\n",
    "\n",
    "print(\"Connecting to pipeline server\")\n",
    "token = subprocess.check_output(\"oc whoami -t\", shell=True, text=True).strip()\n",
    "kfp_client = kfp.Client(host=getenv(\"PIPELINES_URL\"),\n",
    "                        existing_token=token,\n",
    "                        verify_ssl=False)\n",
    "\n",
    "# Create a run for the pipeline\n",
    "print(\"Running Pipeline\")\n",
    "kfp_client.create_run_from_pipeline_func(\n",
    "    distance_model_pipeline,\n",
    "    experiment_name=PIPELINE_NAME,\n",
    "    arguments={\n",
    "        \"user_token\": getenv(\"TOKEN\"),\n",
    "        \"fresh_run\": \"\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb0c9c2-e834-4ec9-b36e-f7f2398afc4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6354be-04a1-4f7e-b6d9-5b16b99de484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935c0df2-f3b6-4df6-ab4a-34a3c7d9d57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Pipeline\n",
    "print(\"Compiling Pipeline\")\n",
    "compiler.Compiler().compile(distance_model_pipeline, 'distance_model_pipeline.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7698869b-a38d-42b3-936f-5a937828b2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = kfp_client.upload_pipeline('distance_model_pipeline.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3307a5e1-631c-4b57-b3b3-e6f87b0a0513",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
